Chapter 23 Sorting

    The Sorting Algorithms introduced in this chapter include:  
        Insertion Sort
        Bubble Sort
        Merge Sort
        Quick Sort
        Bucket Sort
        Radix Sort
        External Sort

    1. Insertion Sort
        "The insertion-sort algorithm sorts a list of values by repeatedly inserting a new element into a sorted sublist until the whole list is sorted." (p.862)

        The Time Complexity of Insertion Sort is: O(n^2), in the best scenorio, it's O(n).

        In the worst case scenorio, the structure is in reverse order, which means that when there are n elements, for element at n-1, it requires n-1 swaps to be at the right position. So, 

            n -> 1

        (n-1) + (n-2) + (n-3) + ... (1) = n(n-1)/2, which means that the time complexity is O(n^2)

    2. Bubble Sort
        "A bubble sort sorts the array in multiple phases. Each pass successively swaps the neighboring elements if the elements are not in order." (p.865)

        The Time Complexity of Bubble Sort is: O(n^n), in the best scenorio, it's O(n). 

        In the worst scenorio, the entire structure is in reverse order, each element requires n-1 swaps to be at the right position. So, 

            n -> 1

        (n-1) + (n-2) + (n-3) + ... (1) = n(n-1)/2, which means that the time complexity is O(n^2)

    3. Merge Sort
        "The algorithm divides the array into two halves and applies a merge sort on each half recursively. After the two halves are sorted, merge them." (p.867)

        The Time Complexity of Merge Sort is O(n logn).

        Explaination:
            The Merge Sort algortihm is a divide-and-conquer algorithm, the divide and conquer operation are essentially copying the two subarrays to the temp arrays, and recursively sorting two subarrays. The merge time for two subarrays takes at least n time to move them to the correct position, so the merge time will be T(n).
            
            N elements are divided by two recursively until there is only 1 element, so it is like a tree with a tree height of log2(n) + 1. As the problem is divided into two part recursively, the recurrence relation is as follows:

                T(n) = T(n/2) + T(n/2) + T(merge)

            The divide operation takes O(1) time regardless of the size of the array, as it is simply identifying the mid point, here c1 is used to denote its constant time for division. The Conquer part recursively sort the two subarrays of n/2 elements each, and the merge time for n elements will be n. 
            
            Eventhough the array (of n elmeents) is divided into subarrays, the total number of elements at each level of recursion is still n in total, so the total merge time for each level of recursion will still be n. So the recurrence relation can be solved by looking at as a tree.

                [h: the height of the tree, n: number of elements to be sorted]

                when h = 1,
                            n           mergeTime = n
                        /       \
                    n/2           n/2   mergeTime = (n)/2 + (n)/2 = n

                when h = 2, mergeTime = (n)/4 + (n)/4 + (n)/4 + (n)/4 = n
                when h = log2(n) + 1, (where there is only one element), mergeTime = n/n * n = n.
                So, the merge time in total will be n at each level regardless of which levle the recursion is at, so the mergeTime in total is n * (log2(n) + 1). 

                As the division takes O(1) time, say, the division happens k time (more specifically 2^log2(n) times), the time for division will be T(k), which is rather insignificant. Taking into account the merge time, the time complexity for merge sort will be:

                    T(n) = T(1) * 2^log2(n) + n * log2(n), therefore, O(n logn).

    4. Quick Sort   

        "The algorithm selects an element, called the pivot, in the array. It divides the array into two parts, so that all the elements in the first part are less than or equal to the pivot and all the elements in the second part are greater than the pivot. The quick sort algorithm is then recursively applied to the first part and then the second part." (p.870)

        Partitioning is the process of selecting the pivot and moving the pivot to the right position where all the elements on the left are less than (or equal to) the pivot and all the elements on the right are greater than (or equal to, depending on the implementation) the pivot.

    5. Heap Sort (In-Place Algorithm)

        "A heap sort uses a binary heap. It first adds all the elements to a heap and then removes the largest elements successively to obtain a sorted list." (p.874) Binary Heap is a "Complete" binary tree with the heap property. Considering the characteristics of a heap, a heap sort is simply constructing a heap and continually removing the root of the heap.
        
            [Root] : It has a root and the left-subtree and right-subtree.
            [Length of A Path] : The length of a path is the number of edges (that connect two elements) in the path. 
            [Depth of A Node] : The depth of a node is the length of the path from the root to the node.
        
        5.1 A binary Heap is a binary tree with theses properties:
            1. [Shape property] : a complete binary tree.
            2. [Heap property] : Each node is greater than or equal to any of its children.

            [Complete Binary Tree] : "A binary tree is complete if each of its levels is full, except that the last level may not be full and all the leaves on the last level are placed leftmost." (p.874)

                e.g., this is not a complete tree, because the not all the leaves are placed leftmost on the last level. This is also not a heap, because 39 is less than 42 which violates the heap property.

                            39
                          /   \
                        32      42
                      /        /   \
                    22       14     33

                e.g., this is a complete tree and a binary heap

                            50
                          /   \
                        32     42
                      /   \    /   
                    22    29  14    

            A heap can be implemented using ArrayList or Array.

            The root is at index:0
                It's children nodes are at index:1 (left) and index:2 (left), which can be interpreted as 2 * 0 + 1 = 1 and 2 * 0 + 2 = 2.

            For the node at index:i
                It's children nodes are at index:2i+1 and index: 2i+2

            e.g., [50, 32, 42, 22, 29, 14]

            root: 50
            1 : 32, 2 : 42
            2*1+1 : 22, 2*1+2 : 29, 2*2+1 : 14 

        5.2 Adding A New Node

            The algorithm is as follows: (p.876)
                
            1      Let the last node be the current node; 
            2   
            3      while (the current node is greater than its parent) {
            4      
            5          Swap the current node with its parent;
            6   
            7          Now the current node is one level up;
            8       }

        5.3 Removing The Root
        
            The algorithm for rebuilding the heap is as follows: (p.876)
            
            1      
            2      Move the last node to replace the root;
            3      
            4      Let the root be the current node;
            5      
            6      while (the current node has children and the current node is
            7      
            8          smaller than one of its children) {
            9          
            10         Swap the current node with the larger of its children;
            11     
            12         Now the current node is one level down;
            13     }

        5.4 Algorithm Analysis

            Since a heap is a binary tree, its characteristics can be calculated as follows:

            If there are n nodes in the Heap,
            h (height) = log2(n),

            5.4.1 Adding All Nodes
                
                To add a new node to the heap, it at most traverses the heap h levels (i.e., it takes h steps) to add the new element to the heap, for n elements, it takes n * log2(n) to add all the elements to a heap.
            
            5.4.2 Removing All Nodes For Sorting

                To remove the root and rebuild the heap, it takes at most h steps traverse through the path from a root to a leaf. So the total time for removing n nodes takes n * log2(n).

            So the total time will be 2 * (n logn), thus, O(n logn)

    6. Bucket Sort (Distribution Sort)

        "Bucket sort and radix sort are efficient for sorting integers." (p.881)

        It is consisdered not possible to sort general elements in a way that can perform better than O(n logn). While if the elements are integers, a bucket sort can be used without comparing the elements.

        The general algotihm is that the element of value i is put into an associated bucket, where each bucket itself can be a list storing similar elements and each bucket is sorted individually.

        Note that the Bucket Sort consists of an important factor - the number of buckets.
            
            The time complexity of the bucket sort is O(n + k), where k refers to the number of buckets.
            The space complexity of the bucket sort is O(n + k).   

        In the worst case scenorio, all the elements are placed in one bucket, the time complexity can be O(n^2) depending on the inner sorting algorithm for each bucket.

        In the best case scenorio, all the elements are already 'sorted' when they are placed in the bucket, so the time complexity is O(n). 

        In the average case scenorio, the time complexity is O(n + k) or O(n).

    7. Radix Sort/ Digital Sort (Based on the Bucket Sort)

        Radix sort can be used when the number of buckets is too great. Radix sort is based on the bucket sort except that it has only 10 buckets. "The idea for the radix sort is to divide the keys into subgroups based on their radix positions. It applies a bucket sort repeatedly for the key values on radix positions, starting from the least-significant position." (p.882)

        Radix position : refers to a specific digit of all elements, which ranges from 0 - 9.

        It first sort elements based on their most-significant digits or the least-significant digits, and then continually sorting the rest of the digits using the bucket sort. 

    8. External Sort

        External Sort is used for the large file that is unable to be stored internally in the memory. The external sort can be broken down into two phases:

            Phase 1 : Create Initial Sorted Segments
                "Repeatedly bring data from the file to an array, sort the array using an internal sorting algorithm, and output the data from the array to a temporary file." (p.883) Each group of data is called segment, the size of the segment will still depend on the size of the memory allocated to the JVM.

                    Essentially, it: 1) loads a segment of data, 2) sort this segment, 3) write this segment to a temp file, 4) and repeatively conduct this process until all data are loaded into the temp file.

            Phase 2 : Merge Sorted Segments
                "Merge a pair of sorted segments (e.g., S1with S2, S3 with S4, c, and so on) into a larger sorted segment and save the new segment into a new temporary file. Continue the same process until only one sorted segment results."(p.884)

                    Essentially, it: 1) splits the temp file into two temp files (through copying from the original temp file) where the first temp file has the first half of segements and another temp file has the rest of the segments, 2) then merge the segments from the two files by comparing the first int of each segment.

        It has a Time Complexity of O(n logn).




        


        


            




    